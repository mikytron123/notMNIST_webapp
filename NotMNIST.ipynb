{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from torch import Tensor, flatten, nn, optim, utils\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "imagefolder = Path(\"notMNIST_small/\")\n",
    "for folder in imagefolder.iterdir():\n",
    "    label = str(folder.name)\n",
    "    filenames = list(map(lambda x:str(x),folder.iterdir()))\n",
    "    tempdf = pd.DataFrame({\"file\":filenames})\n",
    "    tempdf[\"label\"] = label\n",
    "    df = pd.concat([df,tempdf])\n",
    "df = df.reset_index()\n",
    "lb = LabelEncoder()\n",
    "df[\"label\"] = lb.fit_transform(df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labelencoder.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lb,\"labelencoder.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, labels_df:pd.DataFrame, transform=None, target_transform=None):\n",
    "        self.img_labels = labels_df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_labels.iloc[idx][\"file\"]\n",
    "        try:\n",
    "            image = read_image(img_path)\n",
    "        except:\n",
    "            print(idx,img_path)\n",
    "        label = self.img_labels.iloc[idx][\"label\"]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReScale:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def __call__(self,sample):\n",
    "        return sample/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageDataset(df,ReScale())\n",
    "trainds,valds,testds = random_split(ds,[round(0.8*len(ds)),round(0.1*len(ds)),round(0.1*len(ds))+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainds,batch_size=64,pin_memory=True,num_workers=12,persistent_workers=True)\n",
    "testloader = DataLoader(testds,batch_size=64,pin_memory=True,num_workers=12,persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1,6,25,padding=\"same\"),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(2,2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(6,9,15,padding=\"same\"),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(2,2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(9,12,5,padding=\"same\"),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(2,2))\n",
    "        self.fc = nn.Sequential(nn.Linear(108,100),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(100,80),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(80,10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,1,28,28)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = flatten(x,1)\n",
    "        x= self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LightningModule\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model= model\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        x,y = x.cuda().float(),y.cuda()\n",
    "        y = torch.flatten(y)\n",
    "        x = x.reshape(-1,1,28,28)\n",
    "        x_hat = self.model(x)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(x_hat,y)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, *args, **kwargs):\n",
    "        return super().test_step(*args, **kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = 0):\n",
    "        x = batch\n",
    "        if isinstance(batch,list):\n",
    "            x,y=batch\n",
    "        x = x.reshape(-1,1,28,28)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miky/env/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:268: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | Model | 38.4 K\n",
      "--------------------------------\n",
      "38.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "38.4 K    Total params\n",
      "0.154     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67142da3b3a24729a2f8862523e91242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    }
   ],
   "source": [
    "model = LitModel(Model())\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1,max_epochs=400,enable_checkpointing=True,fast_dev_run=False)\n",
    "trainer.fit(model=model, train_dataloaders=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model(testds[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100 * np.sum(np.argmax(predictions,1) == np.argmax(labels,1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtersize1 = 25\n",
    "filtersize2 = 15\n",
    "filtersize3 = 5\n",
    "\n",
    "\n",
    "batch_size = 150\n",
    "\n",
    "depth1 = 6\n",
    "depth2 = 9\n",
    "depth3 = 12\n",
    "\n",
    "numlabels= 10\n",
    "\n",
    "numhidden1 = 100\n",
    "numhidden2 = 80\n",
    "\n",
    "num_labels = 10\n",
    "\n",
    "graph= tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  x = tf.placeholder(tf.float32, shape=(1, image_size, image_size, num_channels),name = \"X\")\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels),name = \"Trainset\")\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels),name = \"Train_labels\")\n",
    "  tf_valid_dataset = tf.constant(X_val,name = \"Validset\")\n",
    "  tf_valid_labels = tf.constant(y_val,name = \"Valid_labels\")\n",
    "  tf_test_dataset = tf.constant(X_test,name = \"Testset\")\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [filtersize1, filtersize1, num_channels, depth1], stddev=0.1),name = \"Layer1W\")\n",
    "  layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth1]),name = \"Layer1B\")\n",
    "    \n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [filtersize2, filtersize2, depth1, depth2], stddev=0.1),name = \"Layer2W\")\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]),name = \"Layer2B\")\n",
    "    \n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [filtersize3, filtersize3, depth2, depth3], stddev=0.1),name = \"Layer3W\")\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth3]),name = \"Layer3B\")\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [16*depth3, numhidden1], stddev=0.1),name = \"Layer4W\")\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[numhidden1]),name = \"Layer4B\")\n",
    "    \n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [numhidden1, numhidden2], stddev=0.1),name = \"Layer5W\")\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[numhidden2]),name = \"Layer5B\")\n",
    "  \n",
    "  layer6_weights = tf.Variable(tf.truncated_normal(\n",
    "      [numhidden2, numlabels], stddev=0.1,name = \"Layer6W\"))\n",
    "  layer6_biases = tf.Variable(tf.constant(1.0, shape=[numlabels]),name = \"Layer6B\")\n",
    "  \n",
    "  saver = tf.train.Saver()\n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME',name = \"Conv1\")\n",
    "    hidden = tf.nn.relu(conv + layer1_biases,name = \"Hidden1\")\n",
    "    pool = tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding = 'SAME',name = \"MaxPool1\")\n",
    "    \n",
    "    conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME',name = \"Conv2\")\n",
    "    hidden = tf.nn.relu(conv + layer2_biases,name = \"Hidden2\")\n",
    "    pool = tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding = 'SAME',name = \"MaxPool2\")\n",
    "    \n",
    "    conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME',name = \"Conv3\")\n",
    "    hidden = tf.nn.relu(conv + layer3_biases,name = \"Hidden3\")\n",
    "    pool = tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding = 'SAME',name = \"MaxPool3\")\n",
    "    \n",
    "    shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases,name = \"Hidden4\")\n",
    "    \n",
    "    logits = tf.matmul(hidden, layer5_weights) + layer5_biases\n",
    "    relu = tf.nn.relu(logits,name = \"Hidden5\")\n",
    "    \n",
    "    return tf.matmul(relu, layer6_weights) + layer6_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  tf.summary.histogram('train_activations', logits)\n",
    "  with tf.name_scope('cross_entropy'):\n",
    "    with tf.name_scope('total'):\n",
    "      loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "      tf.summary.scalar('cross_entropy', loss)\n",
    "        \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.AdamOptimizer(0.005).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  with tf.name_scope('cross_entropy2'):\n",
    "    with tf.name_scope('total2'):\n",
    "        cross_entropy2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=tf_valid_labels, logits=model(tf_valid_dataset)))\n",
    "        tf.summary.scalar('cross_entropy2', cross_entropy2)\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "  y = tf.nn.softmax(model(x))\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f8576e07ef9a052eeebac9107d0ab0bf36d5f128f8c2c26982156b80927ad04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
